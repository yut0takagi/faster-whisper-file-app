import streamlit as st
from faster_whisper import WhisperModel
import tempfile, pathlib, uuid, re, os
import time
import requests

st.set_page_config(page_title="Faster-Whisper Transcriber", page_icon="â±ï¸")

st.title("voice2text with Faster-Whisper")

st.markdown("""
1. éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
2. é€²æ—ãƒãƒ¼ã§å‡¦ç†çŠ¶æ³ã‚’å¯è¦–åŒ–
3. çµæœã‚’ Markdown ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
4. è­°äº‹éŒ²ã¨ã—ã¦ã¾ã¨ã‚ã‚‹ï¼ˆLMStudio APIä½¿ç”¨ï¼‰
""")

# -------- LMStudio APIè¨­å®š --------
st.sidebar.header("âš™ï¸ è¨­å®š")
lmstudio_url = st.sidebar.text_input(
    "LMStudio API URL",
    value="http://localhost:1234/v1/chat/completions",
    help="LMStudioã§APIã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•ã—ãŸéš›ã®URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„"
)
enable_minutes = st.sidebar.checkbox("è­°äº‹éŒ²ã‚’è‡ªå‹•ç”Ÿæˆ", value=False, help="æ–‡å­—èµ·ã“ã—å®Œäº†å¾Œã«è‡ªå‹•ã§è­°äº‹éŒ²ã‚’ç”Ÿæˆã—ã¾ã™")
lmstudio_model = st.sidebar.text_input(
    "LMStudio ãƒ¢ãƒ‡ãƒ«å",
    value="openai/gpt-oss-20b",
    help="LMStudioã§åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«åã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆä¾‹: openai/gpt-oss-20bï¼‰"
)

# -------- åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—ã™ã‚‹æ©Ÿèƒ½ --------
def get_available_models(api_url: str) -> list[str]:
    """LMStudio APIã‹ã‚‰åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’å–å¾—"""
    try:
        from urllib.parse import urlparse
        parsed = urlparse(api_url)
        base_url = f"{parsed.scheme}://{parsed.netloc}"
        models_url = f"{base_url}/v1/models"
        
        response = requests.get(models_url, timeout=5)
        response.raise_for_status()
        result = response.json()
        
        if "data" in result:
            return [model["id"] for model in result["data"]]
        return []
    except Exception:
        return []

if st.sidebar.button("ğŸ“‹ åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—"):
    with st.sidebar:
        with st.spinner("ãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’å–å¾—ä¸­..."):
            from urllib.parse import urlparse
            parsed = urlparse(lmstudio_url)
            base_url = f"{parsed.scheme}://{parsed.netloc}"
            models = get_available_models(lmstudio_url)
            if models:
                st.success(f"åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«:\n" + "\n".join(f"- {m}" for m in models))
                if not lmstudio_model and models:
                    st.info(f"ğŸ’¡ æœ€åˆã®ãƒ¢ãƒ‡ãƒ« '{models[0]}' ã‚’è‡ªå‹•è¨­å®šã—ã¾ã™ã‹ï¼Ÿ")
            else:
                st.warning("ãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚æ‰‹å‹•ã§ãƒ¢ãƒ‡ãƒ«åã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

# -------- APIæ¥ç¶šãƒ†ã‚¹ãƒˆæ©Ÿèƒ½ --------
def test_api_connection(api_url: str, model_name: str) -> tuple[bool, str]:
    """LMStudio APIã¸ã®æ¥ç¶šã‚’ãƒ†ã‚¹ãƒˆ"""
    if not model_name:
        return False, "âŒ ã‚¨ãƒ©ãƒ¼: ãƒ¢ãƒ‡ãƒ«åãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§ãƒ¢ãƒ‡ãƒ«åã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚"
    
    try:
        # ã¾ãšã€ãƒ™ãƒ¼ã‚¹URLã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹ã‹ç¢ºèª
        from urllib.parse import urlparse
        parsed = urlparse(api_url)
        base_url = f"{parsed.scheme}://{parsed.netloc}"
        
        # ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ç”¨ã®è»½ã„ãƒªã‚¯ã‚¨ã‚¹ãƒˆï¼ˆPOSTãƒ¡ã‚½ãƒƒãƒ‰ã‚’æ˜ç¤ºï¼‰
        headers = {
            "Content-Type": "application/json"
        }
        test_response = requests.post(
            api_url,
            json={
                "model": model_name,
                "messages": [{"role": "user", "content": "test"}],
                "max_tokens": 10
            },
            headers=headers,
            timeout=10
        )
        test_response.raise_for_status()
        result = test_response.json()
        return True, f"âœ… APIæ¥ç¶šæˆåŠŸ\n\nä½¿ç”¨URL: {api_url}\nä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {model_name}\nãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ¡ã‚½ãƒƒãƒ‰: POST"
    except requests.exceptions.ConnectionError as e:
        error_detail = str(e)
        return False, f"""âŒ æ¥ç¶šå¤±æ•—

**ã‚¨ãƒ©ãƒ¼è©³ç´°:**
{error_detail}

**ç¢ºèªäº‹é …:**
1. LMStudioã®ã€ŒLocal Serverã€ã‚¿ãƒ–ã§ã€ŒStart Serverã€ãƒœã‚¿ãƒ³ãŒæŠ¼ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
2. ã‚µãƒ¼ãƒãƒ¼ãŒèµ·å‹•ã—ã¦ã„ã‚‹å ´åˆã€è¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹ãƒãƒ¼ãƒˆç•ªå·ã‚’ç¢ºèªï¼ˆä¾‹: http://localhost:1234ï¼‰
3. ç¾åœ¨ã®API URL: {api_url}
4. ãƒãƒ¼ãƒˆç•ªå·ãŒç•°ãªã‚‹å ´åˆã¯ã€ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®API URLã‚’å¤‰æ›´ã—ã¦ãã ã•ã„

**LMStudioã§ã®ç¢ºèªæ–¹æ³•:**
- å³å´ã®ã€ŒLocal Serverã€ã‚¿ãƒ–ã‚’é–‹ã
- ã€ŒStart Serverã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯
- ã‚µãƒ¼ãƒãƒ¼ãŒèµ·å‹•ã™ã‚‹ã¨ã€URLãŒè¡¨ç¤ºã•ã‚Œã¾ã™ï¼ˆä¾‹: http://localhost:1234ï¼‰
- ãã®URLã« `/v1/chat/completions` ã‚’è¿½åŠ ã—ãŸã‚‚ã®ãŒAPI URLã§ã™"""
    except requests.exceptions.Timeout:
        return False, f"âŒ ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: APIã‚µãƒ¼ãƒãƒ¼ã¸ã®å¿œç­”ãŒã‚ã‚Šã¾ã›ã‚“ï¼ˆ10ç§’ä»¥å†…ã«å¿œç­”ãªã—ï¼‰\n\nä½¿ç”¨URL: {api_url}\n\nã‚µãƒ¼ãƒãƒ¼ãŒèµ·å‹•ã—ã¦ã„ã‚‹ã‹ã€ãƒãƒ¼ãƒˆç•ªå·ãŒæ­£ã—ã„ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
    except requests.exceptions.HTTPError as e:
        error_text = e.response.text[:1000] if e.response.text else "ãƒ¬ã‚¹ãƒãƒ³ã‚¹æœ¬æ–‡ãªã—"
        # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚’æŠ½å‡º
        available_models = []
        if "Your models:" in error_text or "not found" in error_text.lower():
            import re
            # ãƒ¢ãƒ‡ãƒ«åã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¢ã™ï¼ˆä¾‹: openai/gpt-oss-20bï¼‰
            model_pattern = r'([a-zA-Z0-9_-]+/[a-zA-Z0-9_.-]+(?::\d+)?)'
            found_models = re.findall(model_pattern, error_text)
            available_models = [m for m in found_models if '/' in m]
        
        error_msg = f"âŒ HTTPã‚¨ãƒ©ãƒ¼: {e.response.status_code} {e.response.reason}\n\nä½¿ç”¨URL: {api_url}\nä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {model_name}\nãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ¡ã‚½ãƒƒãƒ‰: POST\n\n**ã‚¨ãƒ©ãƒ¼è©³ç´°:**\n{error_text}"
        if available_models:
            error_msg += f"\n\nğŸ’¡ åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«:\n" + "\n".join(f"- {m}" for m in available_models)
            error_msg += "\n\nã‚µã‚¤ãƒ‰ãƒãƒ¼ã§ãƒ¢ãƒ‡ãƒ«åã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚"
        elif e.response.status_code == 400:
            error_msg += "\n\nğŸ’¡ 400 Bad Requestã‚¨ãƒ©ãƒ¼ã®å ´åˆã€ä»¥ä¸‹ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™:\n"
            error_msg += "1. ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®å½¢å¼ãŒæ­£ã—ããªã„\n"
            error_msg += "2. å¿…é ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒæ¬ ã‘ã¦ã„ã‚‹\n"
            error_msg += "3. ãƒ¢ãƒ‡ãƒ«åãŒæ­£ã—ããªã„\n"
            error_msg += "4. LMStudioã®APIãƒãƒ¼ã‚¸ãƒ§ãƒ³ãŒç•°ãªã‚‹\n\n"
            error_msg += "LMStudioã®ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ã€è©³ç´°ãªã‚¨ãƒ©ãƒ¼å†…å®¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
        return False, error_msg
    except requests.exceptions.RequestException as e:
        return False, f"âŒ ã‚¨ãƒ©ãƒ¼: {str(e)}\n\nä½¿ç”¨URL: {api_url}"

if st.sidebar.button("ğŸ”Œ APIæ¥ç¶šã‚’ãƒ†ã‚¹ãƒˆ"):
    with st.sidebar:
        with st.spinner("æ¥ç¶šç¢ºèªä¸­..."):
            success, message = test_api_connection(lmstudio_url, lmstudio_model)
            if success:
                st.success(message)
            else:
                st.error(message)
                st.info("ğŸ’¡ ãƒ’ãƒ³ãƒˆ: LMStudioã®ã€ŒLocal Serverã€ã‚¿ãƒ–ã§è¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹URLã‚’ç¢ºèªã—ã¦ãã ã•ã„")

# -------- ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰ --------
@st.cache_resource(show_spinner=True)
def load_model(size: str):
    try:
        import torch
        if torch.cuda.is_available():
            return WhisperModel(size, device="cuda", compute_type="float16")
    except Exception:
        pass
    return WhisperModel(size, device="cpu", compute_type="int8")

# -------- è­°äº‹éŒ²ç”Ÿæˆé–¢æ•° --------
def generate_minutes(transcript_text: str, api_url: str, model_name: str) -> str:
    """LMStudio APIã‚’ä½¿ã£ã¦è­°äº‹éŒ²ã‚’ç”Ÿæˆ"""
    prompt = f"""ä»¥ä¸‹ã®æ–‡å­—èµ·ã“ã—ãƒ†ã‚­ã‚¹ãƒˆã‚’è­°äº‹éŒ²ã¨ã—ã¦æ•´ç†ã—ã¦ãã ã•ã„ã€‚
ä»¥ä¸‹ã®å½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š

# è­°äº‹éŒ²

## æ—¥æ™‚
ï¼ˆè¨˜è¼‰ãŒã‚ã‚Œã°ï¼‰

## å‡ºå¸­è€…
ï¼ˆè¨˜è¼‰ãŒã‚ã‚Œã°ï¼‰

## è­°é¡Œ
ï¼ˆè¨˜è¼‰ãŒã‚ã‚Œã°ï¼‰

## è­°äº‹å†…å®¹
ï¼ˆè¦ç‚¹ã‚’ã¾ã¨ã‚ã¦ï¼‰

## æ±ºå®šäº‹é …
ï¼ˆè¨˜è¼‰ãŒã‚ã‚Œã°ï¼‰

## ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ 
ï¼ˆè¨˜è¼‰ãŒã‚ã‚Œã°ï¼‰

## ãã®ä»–
ï¼ˆè¨˜è¼‰ãŒã‚ã‚Œã°ï¼‰

---

æ–‡å­—èµ·ã“ã—ãƒ†ã‚­ã‚¹ãƒˆï¼š
{transcript_text}
"""
    
    if not model_name:
        return "âŒ ã‚¨ãƒ©ãƒ¼: ãƒ¢ãƒ‡ãƒ«åãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§LMStudioã®ãƒ¢ãƒ‡ãƒ«åã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚"
    
    try:
        # POSTãƒ¡ã‚½ãƒƒãƒ‰ã‚’æ˜ç¤ºçš„ã«ä½¿ç”¨ï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼ã‚‚æ˜ç¤ºï¼‰
        headers = {
            "Content-Type": "application/json"
        }
        response = requests.post(
            api_url,
            json={
                "model": model_name,
                "messages": [
                    {"role": "user", "content": prompt}
                ],
                "temperature": 0.7,
                "max_tokens": 2000
            },
            headers=headers,
            timeout=120
        )
        response.raise_for_status()
        result = response.json()
        return result["choices"][0]["message"]["content"]
    except requests.exceptions.ConnectionError as e:
        return f"""âŒ ã‚¨ãƒ©ãƒ¼: LMStudio APIã¸ã®æ¥ç¶šã«å¤±æ•—ã—ã¾ã—ãŸã€‚

**ã‚¨ãƒ©ãƒ¼å†…å®¹:**
{str(e)}

**ç¢ºèªäº‹é …:**
1. LMStudioã‚’é–‹ã„ã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„
2. LMStudioã®ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‹ã‚‰ã€ŒStart Serverã€ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦APIã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•ã—ã¦ãã ã•ã„
3. ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ã€ŒğŸ”Œ APIæ¥ç¶šã‚’ãƒ†ã‚¹ãƒˆã€ãƒœã‚¿ãƒ³ã§æ¥ç¶šã‚’ç¢ºèªã—ã¦ãã ã•ã„
4. API URLãŒæ­£ã—ã„ã‹ç¢ºèªã—ã¦ãã ã•ã„ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: http://localhost:1234/v1/chat/completionsï¼‰
5. ãƒãƒ¼ãƒˆç•ªå·ãŒå¤‰æ›´ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€LMStudioã®è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„

**LMStudioã§ã®APIã‚µãƒ¼ãƒãƒ¼èµ·å‹•æ–¹æ³•:**
- LMStudioã‚’é–‹ã
- ä¸Šéƒ¨ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‹ã‚‰ã€ŒServerã€â†’ã€ŒStart Serverã€ã‚’é¸æŠ
- ã¾ãŸã¯ã€å³å´ã®ã€ŒLocal Serverã€ã‚¿ãƒ–ã§ã€ŒStart Serverã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯"""
    except requests.exceptions.Timeout:
        return "âŒ ã‚¨ãƒ©ãƒ¼: APIã‚µãƒ¼ãƒãƒ¼ã¸ã®å¿œç­”ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸã€‚LMStudioãŒå¿œç­”ã—ã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
    except requests.exceptions.HTTPError as e:
        error_text = e.response.text[:1000] if e.response.text else "ãƒ¬ã‚¹ãƒãƒ³ã‚¹æœ¬æ–‡ãªã—"
        # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚’æŠ½å‡º
        available_models = []
        if "Your models:" in error_text or "not found" in error_text.lower():
            import re
            model_pattern = r'([a-zA-Z0-9_-]+/[a-zA-Z0-9_.-]+(?::\d+)?)'
            found_models = re.findall(model_pattern, error_text)
            available_models = [m for m in found_models if '/' in m]
        
        error_msg = f"âŒ HTTPã‚¨ãƒ©ãƒ¼: {e.response.status_code} {e.response.reason}\n\nä½¿ç”¨URL: {api_url}\nä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {model_name}\n\n**ã‚¨ãƒ©ãƒ¼è©³ç´°:**\n{error_text}"
        if available_models:
            error_msg += f"\n\nğŸ’¡ åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«:\n" + "\n".join(f"- {m}" for m in available_models)
            error_msg += "\n\nã‚µã‚¤ãƒ‰ãƒãƒ¼ã§æ­£ã—ã„ãƒ¢ãƒ‡ãƒ«åã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚"
        elif e.response.status_code == 400:
            error_msg += "\n\nğŸ’¡ 400 Bad Requestã‚¨ãƒ©ãƒ¼ã®å ´åˆã€ä»¥ä¸‹ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™:\n"
            error_msg += "1. ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®å½¢å¼ãŒæ­£ã—ããªã„\n"
            error_msg += "2. å¿…é ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒæ¬ ã‘ã¦ã„ã‚‹\n"
            error_msg += "3. ãƒ¢ãƒ‡ãƒ«åãŒæ­£ã—ããªã„\n"
            error_msg += "4. LMStudioã®APIãƒãƒ¼ã‚¸ãƒ§ãƒ³ãŒç•°ãªã‚‹\n\n"
            error_msg += "LMStudioã®ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ã€è©³ç´°ãªã‚¨ãƒ©ãƒ¼å†…å®¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
        return error_msg
    except requests.exceptions.RequestException as e:
        # HTTPErrorä»¥å¤–ã®RequestExceptionã®å ´åˆ
        error_detail = str(e)
        if hasattr(e, 'response') and e.response is not None:
            error_detail += f"\n\nãƒ¬ã‚¹ãƒãƒ³ã‚¹: {e.response.text[:500]}"
        return f"âŒ ã‚¨ãƒ©ãƒ¼: LMStudio APIã¸ã®æ¥ç¶šã«å¤±æ•—ã—ã¾ã—ãŸã€‚\n\n**ã‚¨ãƒ©ãƒ¼å†…å®¹:**\n{error_detail}\n\n**ç¢ºèªäº‹é …:**\n1. API URLãŒæ­£ã—ã„ã‹ç¢ºèªã—ã¦ãã ã•ã„\n2. LMStudioã§APIã‚µãƒ¼ãƒãƒ¼ãŒèµ·å‹•ã—ã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„\n3. ãƒ¢ãƒ‡ãƒ«åãŒæ­£ã—ã„ã‹ç¢ºèªã—ã¦ãã ã•ã„"
    except KeyError as e:
        return f"âŒ ã‚¨ãƒ©ãƒ¼: APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®å½¢å¼ãŒäºˆæœŸã—ãªã„ã‚‚ã®ã§ã—ãŸã€‚\n\nã‚¨ãƒ©ãƒ¼å†…å®¹: {str(e)}\n\nãƒ¬ã‚¹ãƒãƒ³ã‚¹: {response.text}"

size_opt = st.selectbox("ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º", ["tiny", "base", "small", "medium", "large"], index=1)
model = load_model(size_opt)

uploaded = st.file_uploader("éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ", type=["wav", "mp3", "m4a", "aac", "flac", "ogg"])

if uploaded:
    suffix = pathlib.Path(uploaded.name).suffix
    with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp:
        tmp.write(uploaded.read())
        tmp_path = pathlib.Path(tmp.name)

    st.info("æ–‡å­—èµ·ã“ã—ä¸­â€¦")
    progress = st.progress(0)
    status   = st.empty()

    segments, info = model.transcribe(str(tmp_path), language="ja")
    total_sec = info.duration
    processed = 0.0
    texts = []

    for seg in segments:
        texts.append(seg.text.strip())
        processed = seg.end  # ç§’
        pct = min(processed / total_sec, 1.0)
        progress.progress(pct)
        status.write(f"{pct*100:5.1f}%  ({processed:.1f}s / {total_sec:.1f}s)")

    progress.progress(1.0)
    status.write("å®Œäº†ï¼")

    raw_text = " ".join(texts)
    sentences = re.split(r"(?<=[ã€‚ï¼ï¼Ÿ!?])", raw_text)
    formatted = "\n\n".join(s.strip() for s in sentences if s.strip())  # æ–‡ï¼‹ç©ºè¡Œ

    md_name = f"transcript_{uuid.uuid4().hex[:8]}.md"
    md_path = pathlib.Path(tempfile.gettempdir()) / md_name
    md_path.write_text(formatted, encoding="utf-8")

    st.text_area("ğŸ“ ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼", formatted, height=300)
    st.download_button("ğŸ“¥ Markdown ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", md_path.read_bytes(), file_name=md_name, mime="text/markdown")

    # è­°äº‹éŒ²ç”Ÿæˆ
    st.divider()
    st.subheader("ğŸ“‹ è­°äº‹éŒ²ç”Ÿæˆ")
    
    if enable_minutes:
        with st.spinner("è­°äº‹éŒ²ã‚’ç”Ÿæˆä¸­..."):
            minutes_text = generate_minutes(formatted, lmstudio_url, lmstudio_model)
    else:
        if st.button("ğŸ” è­°äº‹éŒ²ã‚’ç”Ÿæˆ"):
            with st.spinner("è­°äº‹éŒ²ã‚’ç”Ÿæˆä¸­..."):
                minutes_text = generate_minutes(formatted, lmstudio_url, lmstudio_model)
        else:
            minutes_text = None
    
    if minutes_text:
        st.text_area("ğŸ“‹ è­°äº‹éŒ²ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼", minutes_text, height=400)
        
        minutes_name = f"minutes_{uuid.uuid4().hex[:8]}.md"
        minutes_path = pathlib.Path(tempfile.gettempdir()) / minutes_name
        minutes_path.write_text(minutes_text, encoding="utf-8")
        st.download_button("ğŸ“¥ è­°äº‹éŒ²ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", minutes_path.read_bytes(), file_name=minutes_name, mime="text/markdown")

    os.remove(tmp_path)