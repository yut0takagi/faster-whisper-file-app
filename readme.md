# ğŸ§ Faster-Whisper File Transcriber

A lightweight Streamlit app that lets anyone **upload an audio file and receive a sentenceâ€‘split Japanese transcription in Markdown format**.
Runs entirely on your local machineâ€”no keys, no OpenAI API, no server fees.

---

## âœ¨ Features

| Feature                | Description                                                                                                         |
| ---------------------- | ------------------------------------------------------------------------------------------------------------------- |
| **ğŸ“¤ Fileâ€¯upload**     | Accepts `.wav`, `.mp3`, `.m4a`, `.aac`, `.flac`, `.ogg`                                                             |
| **ğŸš€ Fasterâ€‘Whisper**  | Uses [fasterâ€‘whisper](https://github.com/guillaumekln/faster-whisper) for 2â€‘3Ã— CPU speedâ€‘up (and optional GPU FP16) |
| **ğŸ“Š Progress bar**    | Realâ€‘time progress (% & processed seconds) while transcribing                                                       |
| **ğŸ”€ Model chooser**   | Select `tiny` / `base` / `small` / `medium` / `large` before running                                                |
| **ğŸ“ Markdown output** | Sentences split on `ã€‚ï¼ï¼Ÿ!?`, converted toÂ `.md` and downloadable                                                     |
| **ğŸ—‘ï¸ AutoÂ cleanup**   | Temporary audio files deleted after each run                                                                        |

---

## ğŸ–¥ï¸ Demo (GIF)

![demo gif](docs/demo.png)

---

## ğŸš€ QuickÂ Start

```bash
# 1. Clone
git clone https://github.com/yut0takagi/faster-whisper-file-app.git
cd faster-whisper-file-app

# 2. Install deps (CPUâ€‘only)
pip install -r requirements.txt

#  â””â”€â”€ want GPU?  Make sure torch+CUDA is installed first, then:
#      pip install "faster-whisper[gpu]"

# 3. Launch the app (portÂ 8501)
streamlit run whisper_file_app.py
```

> **Tip:** First run downloads the model (\~75â€¯MB for `base`).  Subsequent runs are instant.

---

## ğŸ³ Docker

Build once, then run on port 8501:

```bash
# Build
docker build -t faster-whisper-file-app .

# Run (CPU)
docker run --rm -p 8501:8501 \
  -v fw_cache:/root/.cache \
  --name fw-app faster-whisper-file-app

# Optional: Run with NVIDIA GPU
# Requires recent NVIDIA drivers + nvidia-container-toolkit
docker run --rm -p 8501:8501 \
  --gpus all \
  -v fw_cache:/root/.cache \
  --name fw-app-gpu faster-whisper-file-app
```

Or use Compose:

```bash
docker compose up --build
# then visit http://localhost:8501
```

Models and tokenizer files are cached under `/root/.cache` and persisted via the named volume (`fw_cache` or `model_cache` in compose).

---

## ğŸ“‚ ProjectÂ Structure

```
â”œâ”€ whisper_file_app.py   # Streamlit UI + transcribe logic
â”œâ”€ requirements.txt      # fasterâ€‘whisper, streamlit, etc.
â”œâ”€ README.md             # â† you are here
â””â”€ docs/
   â””â”€ demo.gif           # optional demo screenshot
```

---

## âš™ï¸ EnvironmentÂ Notes

| Hardware               | Recommended `compute_type`          |
| ---------------------- | ----------------------------------- |
| CPU only               | `int8` (autoâ€‘selected)              |
| NVIDIA GPU             | `float16` (auto if CUDA present)    |
| AppleÂ SiliconÂ Mâ€‘series | works via CPU `int8` (no Metal yet) |

If fasterâ€‘whisper cannot detect a GPU, it silently falls back to `int8` CPU and shows a small warning; performance remains usable for 1â€‘10Â min files.

---

## âœ‚ï¸ Sentence Split Logic

```python
sentences = re.split(r"(?<=[ã€‚ï¼ï¼Ÿ!?])", raw_text)
formatted  = "\n\n".join(s.strip() for s in sentences if s.strip())
```

Feel free to tweak the regex if you need tighter segmentation.

---

## ğŸ“œ License

MITÂ Â©Â 2025Â \Yuto TAKAGI

---

## ğŸ™ Acknowledgements

* [fasterâ€‘whisper](https://github.com/guillaumekln/faster-whisper) â€“ blazingâ€‘fast Whisper inference
* [Streamlit](https://streamlit.io) â€“ simplest way to build ML frontâ€‘ends
* [OpenAI WhisperÂ models](https://github.com/openai/whisper) â€“ base acoustic/language models
